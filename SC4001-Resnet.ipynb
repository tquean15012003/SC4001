{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb209736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9b4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_v2 import Flowers102Classifier, plot_training_runs, get_train_val_test_loader, FineTuneType, TrainingRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea281211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SIZE: 816\n",
      "VALIDATION SIZE: 204\n",
      "TRAINING SIZE: 6149\n"
     ]
    }
   ],
   "source": [
    "train_loader, validation_loader, test_loader = get_train_val_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e304f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature extraction on a resnet18 backbone for 16 epochs.\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/9w/jvgczv21237cq4w760m_n4vw0000gn/T/ipykernel_30100/3858712684.py\", line 80, in <cell line: 80>\n",
      "    transfer_learning_on_backbone(backbones, feature_extract_epochs=16, fine_tune_epochs=8)\n",
      "  File \"/var/folders/9w/jvgczv21237cq4w760m_n4vw0000gn/T/ipykernel_30100/3858712684.py\", line 33, in transfer_learning_on_backbone\n",
      "    fc.train_multiple_epochs_and_save_best_checkpoint(\n",
      "  File \"/Users/queantran/Desktop/Harry/SC4001/utils.py\", line 202, in train_multiple_epochs_and_save_best_checkpoint\n",
      "  File \"/Users/queantran/Desktop/Harry/SC4001/utils.py\", line 146, in train_one_epoch\n",
      "    loss = criterion(outputs, targets)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/queantran/Desktop/Harry/SC4001/utils.py\", line 98, in forward\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\", line 280, in _forward_impl\n",
      "    x = self.fc(x)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x512 and 2048x102)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/queantran/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def transfer_learning_on_backbone(backbones, feature_extract_epochs, fine_tune_epochs):\n",
    "    \"\"\"Run transfer learning on multiple backbones for this classification task.\n",
    "    The choice of the backbone (pre-trained model) is a hyper-parameter.\n",
    "    \n",
    "    We perform transfer-learning in 2 steps:\n",
    "    1. Feature extraction, which is run for feature_extract_epochs, and\n",
    "    2. Fine-tuning, which is run for fine_tune_epochs.\n",
    "    \n",
    "    We save the model with the best validation accuracy after every epoch.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Let's train the last classification later of the pre-trained model with the\n",
    "    # specified backbone on the Flowers 102 dataset.\n",
    "    \n",
    "    training_runs = {}\n",
    "    for backbone in backbones:\n",
    "        best_cp_path = f'{backbone}_Flowers102_best.pt'\n",
    "        print(f\"Running feature extraction on a {backbone} backbone for {feature_extract_epochs} epochs.\\n\")\n",
    "        fc = Flowers102Classifier(backbone=backbone)\n",
    "        fc.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(fc.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.3)\n",
    "        accuracy = MulticlassAccuracy(num_classes=102, average='micro').to(device)\n",
    "\n",
    "        # First freeze all the weights except for the newly added Linear layer.\n",
    "        fc.fine_tune(FineTuneType.NEW_LAYERS)\n",
    "\n",
    "        best_test_accuracy = 0.0\n",
    "        training_run = TrainingRun()\n",
    "        training_runs[backbone] = training_run\n",
    "\n",
    "        fc.train_multiple_epochs_and_save_best_checkpoint(\n",
    "            train_loader,\n",
    "            validation_loader,\n",
    "            accuracy,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            feature_extract_epochs,\n",
    "            best_cp_path,\n",
    "            training_run,\n",
    "        )\n",
    "        \n",
    "        print(f\"Done with feature extraction for {backbone}-based model. Ran for {feature_extract_epochs} epochs.\")\n",
    "        \n",
    "        best_val_accuracy = fc.get_metrics(\"val\")['accuracy']\n",
    "        print(f\"[{backbone}] Best val accuracy after feature extraction is {best_val_accuracy}\\n\")\n",
    "        print(f\"Running fine-tuning for {fine_tune_epochs} epochs\")\n",
    "\n",
    "        # Set all the parameters to be trainable.\n",
    "        fc.fine_tune(FineTuneType.ALL)\n",
    "\n",
    "        optimizer = torch.optim.Adam(fc.get_optimizer_params(), lr=1e-8)\n",
    "        # Every 2 steps reduce the LR to 70% of the previous value.\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)\n",
    "\n",
    "        fc.train_multiple_epochs_and_save_best_checkpoint(\n",
    "            train_loader,\n",
    "            validation_loader,\n",
    "            accuracy,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            fine_tune_epochs,\n",
    "            best_cp_path,\n",
    "            training_run,\n",
    "        )\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(f\"Accuracy of {backbone}-based pre-trained model with last layer replaced.\")\n",
    "        fc.eval()\n",
    "        fc.evaluate(test_loader, accuracy, 0, \"Val\")\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "    # end for (backbone)\n",
    "    \n",
    "    # Now plot the training runs.\n",
    "    plot_training_runs(training_runs)\n",
    "   \n",
    "    \n",
    "# end def\n",
    "backbones = [\"resnet18\", \"resnet50\", \"resnet152\"]\n",
    "transfer_learning_on_backbone(backbones, feature_extract_epochs=16, fine_tune_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b36fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
